{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praktikum 1 - Dasar Penggunaan NLTK #\n",
    "\n",
    "**NLTK** (https://www.nltk.org) adalah library yang digunakan untuk pengolahan data bahasa manusia.\n",
    "\n",
    "Pada bagian ini, kita belajar cara instalasi NLTK dan beberapa contoh dasar penggunaannya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalasi dan Setup\n",
    "\n",
    "Instalasi nltk cukup sederhana, yaitu hanya menggunakan perintah `pip`\n",
    "\n",
    "### Dari command line atau terminal:\n",
    "> `pip install -U nltk`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bermain dengan nltk di Python\n",
    "\n",
    "Berikut ini adalah beberapa contoh perintah dasar untuk nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization\n",
    "Langkah pertama dalam memproses teks adalah membagi semua bagian komponen (kata & tanda baca) menjadi \"token\". Token ini sangat berguna untuk menemukan pola dan dianggap sebagai langkah dasar untuk stemming dan lemmatization. \n",
    "Tokenisasi nltk ada dua, yaitu: tokenisasi kata dan tokenisasi kalimat. \n",
    "Mari kita lihat contoh masing-masing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Tokenisasi Kata ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alhamdulillah', ',', 'hari', 'ini', 'cuacanya', 'cerah', '.', 'Tapi', ',', 'sore', 'hari', 'hujan']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/oddy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import librari nltk\n",
    "import nltk\n",
    "\n",
    "# download corpus punkt\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Alhamdulillah, hari ini cuacanya cerah. Tapi, sore hari hujan\"\n",
    "print(word_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penjelasan kode ####\n",
    "1. `import nltk` adalah import librari nltk\n",
    "1. `nltk.download('punkt')` adalah download database corpus dari nltk. Proses ini hanya sekali ketika running awal.\n",
    "1. `word_tokenize` adalah modul dari librari NLTK.\n",
    "1. `text` adalah variabel yang menampung data teks tipe string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Tokenisasi Kalimat ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alhamdulillah, hari ini cuacanya cerah.', 'Tapi, sore hari hujan']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Alhamdulillah, hari ini cuacanya cerah. Tapi, sore hari hujan\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output dari tokenisasi kata dan kalimat berbeda. Kita amati perbedaan output berikut:\n",
    "1. Tokenisasi kata: `['Alhamdulillah', ',', 'hari', 'ini', 'cuacanya', 'cerah', '.', 'Tapi', ',', 'sore', 'hari', 'hujan']`, \n",
    "1. Tokenisasi kalimat: `['Alhamdulillah, hari ini cuacanya cerah.', 'Tapi, sore hari hujan']`. \n",
    "\n",
    "Tokenisasi pada nltk sudah bisa membedakan mana kalimat dan mana kata berdasarkan tanda baca."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stopword Removal ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop words** adalah kata-kata yang ingin diabaikan atau difilter dari teks. Kata-kata yang sangat umum seperti 'di', 'yang', dan 'saya' sering digunakan sebagai stopword karena tidak menambahkan banyak arti pada teks itu sendiri. Di sini, kita memanfaatkan fitur stopword removal untuk bahasa Inggris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/oddy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# download corpus stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penjelasan kode ####\n",
    "1. `nltk.download('stopwords')` adalah import corpus stopwords yang akan digunakan untuk filter corpus\n",
    "1. `from nltk.corpus import stopwords` adalah import stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tell', 'me', 'and', 'I', 'forget', '.', 'Teach', 'me', 'and', 'I', 'remember', '.', 'Involve', 'me', 'and', 'I', 'learn']\n"
     ]
    }
   ],
   "source": [
    "benjamin_quote = \"Tell me and I forget. Teach me and I remember. Involve me and I learn\"\n",
    "\n",
    "words = word_tokenize(benjamin_quote)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'why', 'down', 'doesn', \"needn't\", 'haven', 'more', 've', \"hadn't\", 'their', 'above', 'needn', 'just', \"couldn't\", 'the', 'i', 'while', \"wasn't\", 'his', 'before', 'on', 'but', 'll', 'by', \"hasn't\", 'here', 'of', \"won't\", 'shouldn', 'been', 'had', 'and', \"you've\", 'from', 'too', 'that', 's', 'we', 'it', 'there', 'should', 'at', 'o', 'through', \"wouldn't\", 'when', 'was', 'ourselves', 'out', 'didn', 'as', 'him', 'he', 'do', 'can', 'mustn', 'an', 'our', 'or', 'no', 'am', 'once', 'few', 'is', 'against', 'which', 'does', 're', \"didn't\", \"it's\", 'between', 'any', 'me', 'both', 'theirs', 'its', 'over', \"don't\", 'mightn', 'them', 'only', 'will', 'wasn', 'this', 'ain', 'most', 'hers', 'about', 'until', 'they', \"shouldn't\", 'were', 'doing', 'again', 'these', \"weren't\", 'aren', \"isn't\", 'shan', 'other', 'nor', 'did', 'up', 'such', 'own', 'because', 'wouldn', 'where', \"shan't\", \"mightn't\", 'are', 'further', 'ma', 'yourselves', 'having', 'her', \"should've\", 'she', 'in', 'isn', 'whom', \"that'll\", \"she's\", 'a', \"you'll\", 'themselves', 'don', 'all', 'myself', \"doesn't\", 'won', 'yours', 'hasn', 'during', 'what', 'then', 'with', 'than', 'itself', 'if', 'who', \"mustn't\", \"you're\", 'below', 'those', 'not', 'some', 'has', 'how', 'same', 'now', 'into', 'so', 'couldn', 'ours', \"aren't\", \"haven't\", 'be', 'm', 'very', 'weren', 'hadn', 'd', 'each', 'under', 'herself', 'being', 'off', 't', 'have', 'yourself', 'you', \"you'd\", 'y', 'my', 'himself', 'your', 'for', 'to', 'after'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penjelasan kode ####\n",
    "1. `stopwords.words(\"english\")` adalah memilih stopwords yang dipakai dalam bahasa Inggris\n",
    "1. `set(stopwords.words(\"english\"))` adalah mengubah tipe data `list` dari poin 1 ke dalam bentuk tipe `set`.\n",
    "1. `print(stop_words)` adalah mencetak daftar stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tell', 'forget', '.', 'Teach', 'remember', '.', 'Involve', 'learn']\n"
     ]
    }
   ],
   "source": [
    "quote_tanpa_stopwords = []\n",
    "\n",
    "for word in words:\n",
    "    if word.casefold() not in stop_words:\n",
    "        quote_tanpa_stopwords.append(word)\n",
    "        \n",
    "print(quote_tanpa_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penjelasan kode ####\n",
    "1. `quote_tanpa_stopwords` adalah variabel yang dipakai untuk menampung teks tanpa stopwords\n",
    "1. `word.casefold()` digunakan untuk mengabaikan besar-kecilnya huruf (upper atau lower case).\n",
    "1. `quote_tanpa_stopwords.append(word)` digunakan untuk memasukkan kata ke dalam list.\n",
    "\n",
    "Untuk penulisan kode python yang berkaitan dengan list, ada beberapa cara. Cara pertama adalah seperti kode berikut:\n",
    "```python\n",
    "quote_tanpa_stopwords = []\n",
    "\n",
    "for word in words:\n",
    "    if word.casefold() not in stop_words:\n",
    "        quote_tanpa_stopwords.append(word)\n",
    "        \n",
    "print(quote_tanpa_stopwords)\n",
    "```\n",
    "\n",
    "sedangkan cara kedua, adalah sebagai berikut:\n",
    "```python\n",
    "quote_tanpa_stopwords = [\n",
    "    word for word in words if word.casefold() not in stop_words\n",
    "]\n",
    "```\n",
    "Kedua model penulisan list tersebut menghasilkan data dan tipe yang sama. Perbedaannya adalah lebih ke gaya penulisan di mana yang kedua itu terkesan lebih **Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tell', 'forget', '.', 'Teach', 'remember', '.', 'Involve', 'learn']\n"
     ]
    }
   ],
   "source": [
    "quote_tanpa_stopwords = [\n",
    "    word for word in words if word.casefold() not in stop_words\n",
    "]\n",
    "\n",
    "print(quote_tanpa_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oke, berikutnya kita lanjutkan ke praktikum tentang **Stemming** dan **Lemmatization**\n",
    "\n",
    "## 3. Stemming ##\n",
    "\n",
    "**Stemming** adalah salah satu bagian dari pemrosesan teks di mana kata direduksi untuk diambil kata dasarnya. Misalnya, kata \"helping\" dan \"helper\" memiliki akar kata \"help\". Stemming memungkinkan Anda untuk membidik arti dasar sebuah kata daripada semua detil tentang bagaimana kata itu digunakan. NLTK memiliki lebih dari satu stemmer, tetapi, di sini kita menggunakan **Stemmer Porter**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['studi', 'love', 'lovingli', 'love', 'lover', 'love', 'repeatedli']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "teks_untuk_stemming = \"studying loving lovingly loved lover lovely repeatedly\"\n",
    "\n",
    "token_teks_untuk_stemming = word_tokenize(teks_untuk_stemming)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in token_teks_untuk_stemming]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penjelasan kode ####\n",
    "1. `from nltk.stem import PorterStemmer` adalah import PorterStemmer untuk stemming \n",
    "1. `teks_untuk_stemming` adalah variabel yang berisi kata yang akan di-stemming\n",
    "1. `stemmer = PorterStemmer()` adalah membuat object dari stemmer\n",
    "1. `stemmed_words` adalah variabel yang berisi kata-kata yang sudah di-stemming\n",
    "1. `stemmer.stem(word)` digunakan untuk stemming per token atau per kata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari kode sebelumnya, outputnya adalah:\n",
    "`['studi', 'love', 'lovingli', 'love', 'lover', 'love', 'repeatedli']`\n",
    "\n",
    "Selain PorterStemmer, NLTK juga memiliki beberapa jenis stemmer yang lain seperti: LancasterStemmer, SnowballStemmer, dan ARLSTem.\n",
    "\n",
    "Mari kita bahas satu per satu perbedaan dari masing-masing stemmer. Langkah-langkahnya adalah\n",
    "1. pertama adalah import librari Stemmer-nya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librari\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. kedua, membuat variabel untuk teks yang akan distemming dan mengubahnya menjadi bentuk token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "teks_untuk_stemming = \"studying loving lovingly loved lover lovely repeatedly\"\n",
    "token_teks_untuk_stemming = word_tokenize(teks_untuk_stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ketiga adalah membuat object dari masing-masing Stemmer satu per satu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "porterStemmer = PorterStemmer()\n",
    "lancasterStemmer = LancasterStemmer()\n",
    "snowballStemmer = SnowballStemmer(language=\"english\")\n",
    "arslStemmer = ARLSTem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. keempat adalah melakukan stemming pada token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmed_words = []\n",
    "lancaster_stemmed_words = []\n",
    "snowball_stemmed_words = []\n",
    "\n",
    "for word in token_teks_untuk_stemming:\n",
    "    porter_stemmed_words.append(porterStemmer.stem(word))\n",
    "    lancaster_stemmed_words.append(lancasterStemmer.stem(word))\n",
    "    snowball_stemmed_words.append(snowballStemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter:  ['studi', 'love', 'lovingli', 'love', 'lover', 'love', 'repeatedli']\n",
      "Lancaster:  ['study', 'lov', 'lov', 'lov', 'lov', 'lov', 'rep']\n",
      "Snowball:  ['studi', 'love', 'love', 'love', 'lover', 'love', 'repeat']\n"
     ]
    }
   ],
   "source": [
    "print(\"Porter: \",porter_stemmed_words)\n",
    "print(\"Lancaster: \", lancaster_stemmed_words)\n",
    "print(\"Snowball: \",snowball_stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penjelasan kode ####\n",
    "1. `from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer` adalah import langsung 3 Stemmer \n",
    "1. Inisiasi variabel\n",
    "```python\n",
    "porterStemmer = PorterStemmer()\n",
    "lancasterStemmer = LancasterStemmer()\n",
    "snowballStemmer = SnowballStemmer(language=\"english\")\n",
    "``` \n",
    "adalah pembuatan object (instantiate) dari masing-masing Stemmer\n",
    "1. Variabel penampung kata yang di-stemming\n",
    "```python\n",
    "porter_stemmed_words = []\n",
    "lancaster_stemmed_words = []\n",
    "snowball_stemmed_words = []\n",
    "```\n",
    "adalah inisialisasi variabel kosong dengan tipe list\n",
    "1. Mencetak hasil tiap-tiap stemming\n",
    "```python\n",
    "print(\"Porter: \",porter_stemmed_words)\n",
    "print(\"Lancaster: \", lancaster_stemmed_words)\n",
    "print(\"Snowball: \",snowball_stemmed_words)\n",
    "```\n",
    "digunakan untuk mencetak hasil stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pertanyaan ###\n",
    "Dari hasil stemming masing-masing teknik, apa yang bisa kalian simpulkan? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lemmatization ##\n",
    "\n",
    "Berbeda dengan stemming, lemmatization lebih dari sekadar pengurangan kata, dan mempertimbangkan kosakata lengkap bahasa serta susunan morfologi kata-kata. Lemma dari 'was' adalah 'be' dan lemma dari 'mice' adalah 'mouse'. Selanjutnya, lemma dari 'meeting' mungkin 'meet' atau 'meeting' tergantung pada penggunaannya dalam sebuah kalimat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/oddy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contoh lemma dari kata 'scarves'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scarf\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"scarves\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari hasil print pada kode sebelumnya, lemma dari 'scarves' menghasilkan 'scarf'. **Lemma mengubah kata ke bentuk asalnya**. Mari kita coba lagi lemma dengan contoh kalimat berikut ini,\"*The striped bats are hanging on their feet for best*\". Di sini, kita mencoba pengaruh lemma terhadap kalimat yang di-stemming dengan tanpa stemming menggunakan teknik Porter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "kalimat = \"The striped bats are hanging on their feet in a dangerous place\"\n",
    "kalimat_token = word_tokenize(kalimat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Dengan PorterStemmer ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming tanpa Lemma:  ['the', 'stripe', 'bat', 'are', 'hang', 'on', 'their', 'feet', 'in', 'a', 'danger', 'place']\n",
      "Stemming dan Lemma:  ['the', 'stripe', 'bat', 'are', 'hang', 'on', 'their', 'foot', 'in', 'a', 'danger', 'place']\n"
     ]
    }
   ],
   "source": [
    "kalimat_token_stemmed = [porterStemmer.stem(word) for word in kalimat_token]\n",
    "kalimat_token_lemmatized_stemmed = [lemmatizer.lemmatize(word) for word in kalimat_token_stemmed]\n",
    "\n",
    "print(\"Stemming tanpa Lemma: \",kalimat_token_stemmed)\n",
    "print(\"Stemming dan Lemma: \",kalimat_token_lemmatized_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Tanpa PorterStemmer ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma tanpa Stemming:  ['The', 'striped', 'bat', 'are', 'hanging', 'on', 'their', 'foot', 'in', 'a', 'dangerous', 'place']\n"
     ]
    }
   ],
   "source": [
    "kalimat_token_lemmatized_non_stemmed = [lemmatizer.lemmatize(word) for word in kalimat_token]\n",
    "\n",
    "print(\"Lemma tanpa Stemming: \",kalimat_token_lemmatized_non_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penjelasan kode ####\n",
    "1. `from nltk.stem import WordNetLemmatizer` adalah mengimpor WordNetLemmatizer sebagai fitur Lemmatization\n",
    "1. `nltk.download('wordnet')` adalah corpus yang digunakan untuk lemma\n",
    "1. `lemmatizer = WordNetLemmatizer()` adalah pembuatan (instantiate) object WordNetLemmatizer\n",
    "1. `lemmatizer.lemmatize(\"scarves\")` adalah contoh penggunaan lemma pada kata \"scarves\" yang menghasilkan bentuk dasar katanya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demikian praktikum 1, semoga lancar dan penuh berkah belajarnya.\n",
    "\n",
    "## 5. Referensi ##\n",
    "\n",
    "1. N. Indurkhya and F. J. Damerau, Handbook of Natural Language Processing. CRC Press, 2010.\n",
    "1. https://realpython.com/nltk-nlp-python/\n",
    "1. https://www.nltk.org/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
