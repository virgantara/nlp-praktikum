{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praktikum 1 - Dasar Penggunaan NLTK #\n",
    "\n",
    "**NLTK** (https://www.nltk.org) adalah library yang digunakan untuk pengolahan data bahasa manusia.\n",
    "\n",
    "Pada bagian ini, kita belajar cara instalasi NLTK dan beberapa contoh dasar penggunaannya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalasi dan Setup\n",
    "\n",
    "Instalasi nltk cukup sederhana, yaitu hanya menggunakan perintah `pip`\n",
    "\n",
    "## Dari command line atau terminal: ##\n",
    "> `pip install -U nltk`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bermain dengan nltk di Python\n",
    "\n",
    "Berikut ini adalah beberapa contoh perintah dasar untuk nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Langkah pertama dalam memproses teks adalah membagi semua bagian komponen (kata & tanda baca) menjadi \"token\". Token ini sangat berguna untuk menemukan pola dan dianggap sebagai langkah dasar untuk stemming dan lemmatization. \n",
    "Tokenisasi nltk ada dua, yaitu: tokenisasi kata dan tokenisasi kalimat. \n",
    "Mari kita lihat contoh masing-masing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisasi Kata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alhamdulillah', ',', 'hari', 'ini', 'cuacanya', 'cerah', '.', 'Tapi', ',', 'sore', 'hari', 'hujan']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/oddy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import librari nltk\n",
    "import nltk\n",
    "\n",
    "# download corpus punkt\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Alhamdulillah, hari ini cuacanya cerah. Tapi, sore hari hujan\"\n",
    "print(word_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penjelasan kode ####\n",
    "1. `import nltk` adalah import librari nltk\n",
    "1. `nltk.download('punkt')` adalah download database corpus dari nltk. Proses ini hanya sekali ketika running awal.\n",
    "1. `word_tokenize` adalah modul dari librari NLTK.\n",
    "1. `text` adalah variabel yang menampung data teks tipe string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisasi Kalimat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alhamdulillah, hari ini cuacanya cerah.', 'Tapi, sore hari hujan']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Alhamdulillah, hari ini cuacanya cerah. Tapi, sore hari hujan\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output dari tokenisasi kata dan kalimat berbeda. Kita amati perbedaan output berikut:\n",
    "1. Tokenisasi kata: `['Alhamdulillah', ',', 'hari', 'ini', 'cuacanya', 'cerah', '.', 'Tapi', ',', 'sore', 'hari', 'hujan']`, \n",
    "1. Tokenisasi kalimat: `['Alhamdulillah, hari ini cuacanya cerah.', 'Tapi, sore hari hujan']`. \n",
    "\n",
    "Tokenisasi pada nltk sudah bisa membedakan mana kalimat dan mana kata berdasarkan tanda baca."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop words** adalah kata-kata yang ingin diabaikan atau difilter dari teks. Kata-kata yang sangat umum seperti 'di', 'yang', dan 'saya' sering digunakan sebagai stopword karena tidak menambahkan banyak arti pada teks itu sendiri. Di sini, kita memanfaatkan fitur stopword removal untuk bahasa Inggris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/oddy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# download corpus stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penjelasan kode ####\n",
    "1. `nltk.download('stopwords')` adalah import corpus stopwords yang akan digunakan untuk filter corpus\n",
    "1. `from nltk.corpus import stopwords` adalah import stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tell', 'me', 'and', 'I', 'forget', '.', 'Teach', 'me', 'and', 'I', 'remember', '.', 'Involve', 'me', 'and', 'I', 'learn']\n"
     ]
    }
   ],
   "source": [
    "benjamin_quote = \"Tell me and I forget. Teach me and I remember. Involve me and I learn\"\n",
    "\n",
    "words = word_tokenize(benjamin_quote)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'with', 'against', 'are', 'be', \"you'd\", 'both', \"wouldn't\", 'herself', 'it', 'haven', 'of', 'itself', 'too', 'if', \"isn't\", 'between', 'because', 'again', \"doesn't\", 'doing', 't', 'theirs', 'other', \"you're\", \"couldn't\", 'by', 'who', 'do', 'nor', 'off', 'more', 'he', 'only', 'me', 'into', 'up', 'should', \"mustn't\", 'any', 'a', 'you', 'such', 'while', \"didn't\", 'didn', 'until', 'm', 'll', 'mightn', 'very', 'ma', 'their', 'weren', \"weren't\", 'hasn', 'them', 'but', 'been', 'below', 'own', 'wouldn', 'were', 'why', 'what', 'hers', 'an', 'in', 'from', 'her', 'further', 'our', 'or', 've', 'whom', 'same', 'to', 'out', 'am', 'himself', 'which', 'few', \"mightn't\", 'she', 'couldn', 'not', 'some', \"wasn't\", \"haven't\", 'yourself', 'on', 'isn', \"shan't\", 'shan', 'they', \"shouldn't\", 'where', 'him', 'aren', 'can', 're', 'when', 'your', 'through', \"it's\", 'is', 'than', 'ourselves', 'themselves', 'yours', 'before', 'had', 'its', 'then', 'his', 's', 'this', \"aren't\", 'now', \"she's\", 'down', 'did', 'these', 'needn', 'the', 'has', 'during', 'yourselves', 'no', \"needn't\", 'and', 'each', 'those', 'over', 'ours', 'as', 'myself', 'above', \"you'll\", 'about', 'hadn', 'there', 'we', 'wasn', 'here', 'once', 'o', 'was', 'does', 'have', 'my', 'so', 'just', 'ain', \"should've\", 'most', 'doesn', 'don', \"hadn't\", 'being', \"don't\", 'i', \"that'll\", \"you've\", 'after', 'will', 'having', 'shouldn', \"won't\", 'mustn', \"hasn't\", 'd', 'that', 'at', 'won', 'y', 'how', 'for', 'all', 'under'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penjelasan kode ####\n",
    "1. `stopwords.words(\"english\")` adalah memilih stopwords yang dipakai dalam bahasa Inggris\n",
    "1. `set(stopwords.words(\"english\"))` adalah mengubah tipe data `list` dari poin 1 ke dalam bentuk tipe `set`.\n",
    "1. `print(stop_words)` adalah mencetak daftar stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tell', 'forget', '.', 'Teach', 'remember', '.', 'Involve', 'learn']\n"
     ]
    }
   ],
   "source": [
    "quote_tanpa_stopwords = []\n",
    "\n",
    "for word in words:\n",
    "    if word.casefold() not in stop_words:\n",
    "        quote_tanpa_stopwords.append(word)\n",
    "        \n",
    "print(quote_tanpa_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penjelasan kode ####\n",
    "1. `quote_tanpa_stopwords` adalah variabel yang dipakai untuk menampung teks tanpa stopwords\n",
    "1. `word.casefold()` digunakan untuk mengabaikan besar-kecilnya huruf (upper atau lower case).\n",
    "1. `quote_tanpa_stopwords.append(word)` digunakan untuk memasukkan kata ke dalam list.\n",
    "\n",
    "Untuk penulisan kode python yang berkaitan dengan list, ada beberapa cara. Cara pertama adalah seperti kode berikut:\n",
    "```python\n",
    "quote_tanpa_stopwords = []\n",
    "\n",
    "for word in words:\n",
    "    if word.casefold() not in stop_words:\n",
    "        quote_tanpa_stopwords.append(word)\n",
    "        \n",
    "print(quote_tanpa_stopwords)\n",
    "```\n",
    "\n",
    "sedangkan cara kedua, adalah sebagai berikut:\n",
    "```python\n",
    "quote_tanpa_stopwords = [\n",
    "    word for word in words if word.casefold() not in stop_words\n",
    "]\n",
    "```\n",
    "Kedua model penulisan list tersebut menghasilkan data dan tipe yang sama. Perbedaannya adalah lebih ke gaya penulisan di mana yang kedua itu terkesan lebih **Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tell', 'forget', '.', 'Teach', 'remember', '.', 'Involve', 'learn']\n"
     ]
    }
   ],
   "source": [
    "quote_tanpa_stopwords = [\n",
    "    word for word in words if word.casefold() not in stop_words\n",
    "]\n",
    "\n",
    "print(quote_tanpa_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oke, berikutnya kita lanjutkan ke praktikum tentang **Stemming** dan **Lemmatization**\n",
    "\n",
    "## Stemming\n",
    "\n",
    "**Stemming** adalah salah satu bagian dari pemrosesan teks di mana kata direduksi untuk diambil kata dasarnya. Misalnya, kata \"helping\" dan \"helper\" memiliki akar kata \"help\". Stemming memungkinkan Anda untuk membidik arti dasar sebuah kata daripada semua detil tentang bagaimana kata itu digunakan. NLTK memiliki lebih dari satu stemmer, tetapi, di sini kita menggunakan **Stemmer Porter**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['studi', 'love', 'lovingli', 'love', 'lover', 'love', 'repeatedli']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "teks_untuk_stemming = \"studying loving lovingly loved lover lovely repeatedly\"\n",
    "\n",
    "token_teks_untuk_stemming = word_tokenize(teks_untuk_stemming)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in token_teks_untuk_stemming]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penjelasan kode ####\n",
    "1. `from nltk.stem import PorterStemmer` adalah import PorterStemmer untuk stemming \n",
    "1. `teks_untuk_stemming` adalah variabel yang berisi kata yang akan di-stemming\n",
    "1. `stemmer = PorterStemmer()` adalah membuat object dari stemmer\n",
    "1. `stemmed_words` adalah variabel yang berisi kata-kata yang sudah di-stemming\n",
    "1. `stemmer.stem(word)` digunakan untuk stemming per token atau per kata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari kode sebelumnya, outputnya adalah:\n",
    "`['studi', 'love', 'lovingli', 'love', 'lover', 'love', 'repeatedli']`\n",
    "\n",
    "Selain PorterStemmer, NLTK juga memiliki beberapa jenis stemmer yang lain seperti: LancasterStemmer, SnowballStemmer, dan ARLSTem.\n",
    "\n",
    "Mari kita bahas satu per satu perbedaan dari masing-masing stemmer. Langkah-langkahnya adalah\n",
    "1. pertama adalah import librari Stemmer-nya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librari\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. kedua, membuat variabel untuk teks yang akan distemming dan mengubahnya menjadi bentuk token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "teks_untuk_stemming = \"studying loving lovingly loved lover lovely repeatedly\"\n",
    "token_teks_untuk_stemming = word_tokenize(teks_untuk_stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ketiga adalah membuat object dari masing-masing Stemmer satu per satu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "porterStemmer = PorterStemmer()\n",
    "lancasterStemmer = LancasterStemmer()\n",
    "snowballStemmer = SnowballStemmer(language=\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. keempat adalah melakukan stemming pada token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmed_words = []\n",
    "lancaster_stemmed_words = []\n",
    "snowball_stemmed_words = []\n",
    "\n",
    "for word in token_teks_untuk_stemming:\n",
    "    porter_stemmed_words.append(porterStemmer.stem(word))\n",
    "    lancaster_stemmed_words.append(lancasterStemmer.stem(word))\n",
    "    snowball_stemmed_words.append(snowballStemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter:  ['studi', 'love', 'lovingli', 'love', 'lover', 'love', 'repeatedli']\n",
      "Lancaster:  ['study', 'lov', 'lov', 'lov', 'lov', 'lov', 'rep']\n",
      "Snowball:  ['studi', 'love', 'love', 'love', 'lover', 'love', 'repeat']\n"
     ]
    }
   ],
   "source": [
    "print(\"Porter: \",porter_stemmed_words)\n",
    "print(\"Lancaster: \", lancaster_stemmed_words)\n",
    "print(\"Snowball: \",snowball_stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penjelasan kode ####\n",
    "1. `from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer` adalah import langsung 3 Stemmer \n",
    "1. Inisiasi variabel\n",
    "```python\n",
    "porterStemmer = PorterStemmer()\n",
    "lancasterStemmer = LancasterStemmer()\n",
    "snowballStemmer = SnowballStemmer(language=\"english\")\n",
    "``` \n",
    "adalah pembuatan object (instantiate) dari masing-masing Stemmer\n",
    "1. Variabel penampung kata yang di-stemming\n",
    "```python\n",
    "porter_stemmed_words = []\n",
    "lancaster_stemmed_words = []\n",
    "snowball_stemmed_words = []\n",
    "```\n",
    "adalah inisialisasi variabel kosong dengan tipe list\n",
    "1. Mencetak hasil tiap-tiap stemming\n",
    "```python\n",
    "print(\"Porter: \",porter_stemmed_words)\n",
    "print(\"Lancaster: \", lancaster_stemmed_words)\n",
    "print(\"Snowball: \",snowball_stemmed_words)\n",
    "```\n",
    "digunakan untuk mencetak hasil stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pertanyaan ###\n",
    "Dari hasil stemming masing-masing teknik, apa yang bisa kalian simpulkan? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization ##\n",
    "\n",
    "Berbeda dengan stemming, lemmatization lebih dari sekadar pengurangan kata, dan mempertimbangkan kosakata lengkap bahasa serta susunan morfologi kata-kata. Lemma dari 'was' adalah 'be' dan lemma dari 'mice' adalah 'mouse'. Selanjutnya, lemma dari 'meeting' mungkin 'meet' atau 'meeting' tergantung pada penggunaannya dalam sebuah kalimat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/oddy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contoh lemma dari kata 'scarves'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scarf\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"scarves\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari hasil print pada kode sebelumnya, lemma dari 'scarves' menghasilkan 'scarf'. **Lemma mengubah kata ke bentuk asalnya**. Mari kita coba lagi lemma dengan contoh kalimat berikut ini,\"*The striped bats are hanging on their feet for best*\". Di sini, kita mencoba pengaruh lemma terhadap kalimat yang di-stemming dengan tanpa stemming menggunakan teknik Porter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "kalimat = \"The striped bats are hanging on their feet in a dangerous place\"\n",
    "kalimat_token = word_tokenize(kalimat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dengan PorterStemmer ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming tanpa Lemma:  ['the', 'stripe', 'bat', 'are', 'hang', 'on', 'their', 'feet', 'in', 'a', 'danger', 'place']\n",
      "Stemming dan Lemma:  ['the', 'stripe', 'bat', 'are', 'hang', 'on', 'their', 'foot', 'in', 'a', 'danger', 'place']\n"
     ]
    }
   ],
   "source": [
    "kalimat_token_stemmed = [porterStemmer.stem(word) for word in kalimat_token]\n",
    "kalimat_token_lemmatized_stemmed = [lemmatizer.lemmatize(word) for word in kalimat_token_stemmed]\n",
    "\n",
    "print(\"Stemming tanpa Lemma: \",kalimat_token_stemmed)\n",
    "print(\"Stemming dan Lemma: \",kalimat_token_lemmatized_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tanpa PorterStemmer ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma tanpa Stemming:  ['The', 'striped', 'bat', 'are', 'hanging', 'on', 'their', 'foot', 'in', 'a', 'dangerous', 'place']\n"
     ]
    }
   ],
   "source": [
    "kalimat_token_lemmatized_non_stemmed = [lemmatizer.lemmatize(word) for word in kalimat_token]\n",
    "\n",
    "print(\"Lemma tanpa Stemming: \",kalimat_token_lemmatized_non_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penjelasan kode ####\n",
    "1. `from nltk.stem import WordNetLemmatizer` adalah mengimpor WordNetLemmatizer sebagai fitur Lemmatization\n",
    "1. `nltk.download('wordnet')` adalah corpus yang digunakan untuk lemma\n",
    "1. `lemmatizer = WordNetLemmatizer()` adalah pembuatan (instantiate) object WordNetLemmatizer\n",
    "1. `lemmatizer.lemmatize(\"scarves\")` adalah contoh penggunaan lemma pada kata \"scarves\" yang menghasilkan bentuk dasar katanya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demikian praktikum 1, semoga lancar dan penuh berkah belajarnya.\n",
    "\n",
    "## Referensi ##\n",
    "\n",
    "1. N. Indurkhya and F. J. Damerau, Handbook of Natural Language Processing. CRC Press, 2010.\n",
    "1. https://realpython.com/nltk-nlp-python/\n",
    "1. https://www.nltk.org/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas Praktikum 1 #\n",
    "\n",
    "## Judul Tugas ##\n",
    "Text Preprocessing\n",
    "\n",
    "## Deskripsi ##\n",
    "Gunakan beragam teknik preprocess yang telah dipelajari pada perkuliahan pertemuan ke-3 dan praktikum ke-1 untuk mengolah teks berikut:\n",
    "\n",
    "*South Africa's health minister says there is \"absolutely no need to panic\" over the new coronavirus variant Omicron, despite a surge in cases. \"We have been here before,\" Joe Phaahla added, referring to the Beta variant detected in South Africa last December. South Africa also condemned the travel bans imposed on the country, saying they should be lifted immediately. Omicron has been classed as a \"variant of concern\". Early evidence suggests it has a heightened re-infection risk. The heavily mutated variant was detected in South Africa earlier this month and then reported to the World Health Organization (WHO) last Wednesday. The variant is responsible for most of the infections found in South Africa's most populated province, Gauteng, over the last two weeks. The number of cases of \"appears to be increasing in almost all provinces\" in the country, according to the WHO. Does southern Africa have enough vaccines? South Africans fear impact of new variant measures Covid variants: Do we need new vaccines yet? Africa Live: More on this and other stories from the continent South Africa reported 2,800 new infections on Sunday, a rise from the daily average of 500 in the previous week. Government adviser and epidemiologist Salim Abdool Karim said he expected the number of cases to reach more than 10,000 a day by the end of the week, and for hospitals to come under pressure in the next two to three weeks. Dr Phaahla said he wanted to \"reiterate that there is absolutely no need to panic\" because this \"is no new territory for us\". \"We are now more than 20 months' experienced in terms of Covid-19, various variants and waves,\" he added at a media briefing. Media caption, Cyril Ramaphosa: \"We are deeply disappointed by the decision of several countries to prohibit travel.\" On Monday, Japan became the latest country to reinstate tough border restrictions, banning all foreigners from entering from 30 November. The UK, EU and US are among those who earlier imposed travel bans on South Africa and other regional states. UN Secretary General António Guterres said he was \"deeply concerned\" about the isolation of southern Africa, adding that \"the people of Africa cannot be blamed for the immorally low level of vaccinations available in Africa\". The bans and restrictions have left the plans of a huge number of travellers up in the air. South African Annalee Veysey, who is getting treatment for cancer in South Africa, was expecting to be reunited with her family in the UK early in December. She has been separated from them for the last 15 months because of earlier travel restrictions and her treatment. \"It's almost two years of my life I've missed out with my family. Especially if you've had a journey with cancer, you find what your family means to you,\" she told the BBC, adding that she felt \"desperate\". Travellers at a near empty airportImage source, EPA Image caption, South Africa's main airport in Johannesburg was getting quieter over the weekend as restrictions were taking effect Hannah Day is stuck in Pretoria. She flew to South Africa last week after she got news that her son, who lives there, was in hospital after being bitten by a snake. He is now recovering but Ms Day needs to return to the UK for work. \"I can self-isolate, but I cannot afford to pay for the quarantine,\" she told the BBC. The WHO has warned against countries hastily imposing travel curbs, saying they should look to a \"risk-based and scientific approach\". The world body's Africa director Matshidiso Moeti said on Sunday: \"With the Omicron variant now detected in several regions of the world, putting in place travel bans that target Africa attacks global solidarity.\" However, Rwanda and Angola are among African states that have announced a restriction on flights to and from South Africa. South Africa's foreign ministry spokesman Clayson Monyela described their decision as \"quite regrettable, very unfortunate, and I will even say sad\". In a speech on Sunday, South Africa's President Cyril Ramaphosa said the bans would not be effective in preventing the spread of the variant. \"The only thing the prohibition on travel will do is to further damage the economies of the affected countries and undermine their ability to respond to, and recover from, the pandemic,\" he said. Current regulations in South Africa make it mandatory to wear face coverings in public, and restrict indoor gatherings to 750 people and outdoor gatherings to 2,000. Mr Ramaphosa said South Africa would not impose new restrictions, but would \"undertake broad consultations on making vaccination mandatory for specific activities and locations\". There are no vaccine shortages in South Africa itself, and Mr Ramaphosa urged more people to get jabbed, saying that remained the best way to fight the virus. Health experts said that Gauteng, which includes Johannesburg, had entered a fourth wave, and most hospital admissions were of unvaccinated people. Omicron has now been detected in a number of countries around the world, including the UK, Germany, Australia and Israel. In other developments: China said it would offer 1bn doses of vaccines to African countries on top of the 200m it had already supplied US Covid adviser Anthony Fauci says the government is on \"high alert\" and that spread is inevitable A Czech woman who came back from Namibia recently was confirmed to have the Omicron variant Portugal has detected 13 cases of the variant among players and staff of Lisbon-based Belenenses SAD football club Australia has paused its plans to reopen its borders in light of the Omicron variant*\n",
    "\n",
    "## Penilaian ##\n",
    "1. Program harus dibuat dengan bahasa Jupyter Notebook (10 poin)\n",
    "1. Harus ada fungsi preprocess() dengan parameter teks. Contoh: preprocess(teks) (10 poin)\n",
    "1. Harus ada fungsi get_word_token(teks) (20 poin)\n",
    "1. Harus ada fungsi remove_stopword(teks) (20 poin)\n",
    "1. Harus ada fungsi porter_stem(teks) (20 poin)\n",
    "1. Harus ada fungsi lemma(teks) (10 poin)\n",
    "1. Hitung jumlah kata sebelum dan sesudah preprocessing (10 poin)\n",
    "\n",
    "## Catatan ##\n",
    "1. Pengumpulan tugas praktikum 1 dikumpulkan di Google Classroom dengan nama file TP1_NIM_Nama_Lengkap_Anda.ipynb\n",
    "1. Kesalahan penamaan nama dan format file, berakibat pada penolakan Tugas Praktikum 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
